# -*- coding: utf-8 -*-
"""challenge21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bjflUA1YHJrfISZo6MLNkcL227tj0h4u

## Preprocessing
"""

# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd
application_df = pd.read_csv("https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv")
application_df.head()

# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
#  YOUR CODE GOES HERE
new_application_df = application_df.drop(columns=['EIN','NAME'],axis=1)
new_application_df.head()

new_application_df.nunique()

# Determine the number of unique values in each column.
#  YOUR CODE GOES HERE
new_application_df['APPLICATION_TYPE']



# Look at APPLICATION_TYPE value counts for binning
#  YOUR CODE GOES HERE
new_application_df['APPLICATION_TYPE'].value_counts()

application_types_list = new_application_df['APPLICATION_TYPE'].value_counts()
application_types_list[1]
# application_types_to_replace=[]
# application_types_to_replace =
# for i in range(lenght(application_types_list)):
#   if application_types_list[i] <500:

# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
#  Threshold set to 500

application_types_to_replace = application_types_list.loc[lambda x: x<500].index

# Replace in dataframe
for app in application_types_to_replace:
    new_application_df['APPLICATION_TYPE'] = new_application_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure binning was successful
new_application_df['APPLICATION_TYPE'].value_counts()

classification_to_replace = new_application_df['CLASSIFICATION'].value_counts()
classification_to_replace

# Look at CLASSIFICATION value counts for binning
#  YOUR CODE GOES HERE

# You may find it helpful to look at CLASSIFICATION value counts >1
#  YOUR CODE GOES HERE

# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
#  Threshold = 1000
classifications_to_replace = classification_to_replace.loc[lambda x: x<1000].index
# Replace in dataframe
for cls in classifications_to_replace:
    new_application_df['CLASSIFICATION'] = new_application_df['CLASSIFICATION'].replace(cls,"Other")

# Check to make sure binning was successful
new_application_df['CLASSIFICATION'].value_counts()

# Convert categorical data to numeric with `pd.get_dummies`
#  YOUR CODE GOES HERE

cat_new_application_df = pd.get_dummies(new_application_df)
cat_new_application_df.head()

# Split our preprocessed data into our features and target arrays
y = cat_new_application_df['IS_SUCCESSFUL']

X = cat_new_application_df.drop(['IS_SUCCESSFUL'],axis=1)

# Split the preprocessed data into a training and testing dataset

X_train, X_test, y_train, y_test = train_test_split (X,y, random_state = 69)
#  YOUR CODE GOES HERE

# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

"""## Compile, Train and Evaluate the Model"""

# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE

nn = tf.keras.models.Sequential()

# First hidden layer
# Add our first Dense layer, including the input layer
nn.add(tf.keras.layers.Dense(units=90, activation="relu", input_dim=43))

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=36, activation="relu"))

# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn.summary()

# Compile the model
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
fit_model = nn.fit(X_train_scaled, y_train, epochs=100)

# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Export our model to HDF5 file
#  YOUR CODE GOES HERE
nn.save('chanllenge21L2U190U236.h5')